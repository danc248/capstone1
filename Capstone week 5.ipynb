{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 4.5.11\n",
      "  latest version: 4.8.1\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 4.5.11\n",
      "  latest version: 4.8.1\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 4.5.11\n",
      "  latest version: 4.8.1\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "Libraries installed!\n"
     ]
    }
   ],
   "source": [
    "!conda install -c conda-forge beautifulsoup4 --yes\n",
    "\n",
    "!conda install -c conda-forge geopy --yes\n",
    "\n",
    "!conda install -c conda-forge folium=0.5.0 --yes\n",
    "\n",
    "print('Libraries installed!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "import requests\n",
    "from pandas.io.json import json_normalize\n",
    "import json\n",
    "\n",
    "import requests\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from geopy.geocoders import Nominatim\n",
    "\n",
    "import folium\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as colors\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "print('Libraries imported!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported!\n"
     ]
    }
   ],
   "source": [
    "import urllib.request # open and read URLs\n",
    "\n",
    "import json # handle JSON files\n",
    "from pandas.io.json import json_normalize # tranform JSON file into a pandas dataframe\n",
    "\n",
    "import requests # handle requests\n",
    "import pandas as pd # process data as dataframes with Pandas\n",
    "import numpy as np # handle data in a vectorized manner with NumPy\n",
    "\n",
    "# !conda install -c conda-forge geopy --yes # uncomment this line if you haven't installed the GeoPy geocoding library yet\n",
    "from geopy.geocoders import Nominatim # convert an address into latitude and longitude values\n",
    "\n",
    "# !conda install -c conda-forge folium=0.5.0 --yes # uncomment this line if you haven't installed the Folium library yet\n",
    "import folium # map rendering library\n",
    "\n",
    "# Matplolib plotting library and associated modules\n",
    "import matplotlib.pyplot as plt \n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as colors\n",
    "\n",
    "from sklearn.cluster import KMeans # for K-Means clustering with Scikit-Learn\n",
    "\n",
    "print(\"Libraries imported!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data about Paris geography from https://opendata.paris.fr website\n",
    "\n",
    "with urllib.request.urlopen(\"https://opendata.paris.fr/explore/dataset/arrondissements/download?format=json&timezone=Europe/Berlin&use_labels_for_header=true\") as url:\n",
    "   data = json.loads(url.read())\n",
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tranform the json data into a Pandas dataframe\n",
    "\n",
    "par_nei = json_normalize(data)\n",
    "par_nei.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format the dataframe\n",
    "\n",
    "df_temp1 = par_nei[[\"fields.c_ar\",\"fields.l_aroff\",\"fields.geom_x_y\"]]\n",
    "df_temp2 = pd.DataFrame(par_nei['fields.geom_x_y'].tolist(), columns=[\"Latitude\", \"Longitude\"])\n",
    "df_drop = df_temp1.drop([\"fields.geom_x_y\"], axis = 1)\n",
    "df_temp = pd.concat([df_drop,df_temp2], axis=1)\n",
    "df_temp.columns = [\"Arrondissement\", \"Neighborhood\", \"Latitude\", \"Longitude\"]\n",
    "paris_data = df_temp.sort_values([\"Arrondissement\"]).reset_index(drop=True)\n",
    "\n",
    "# change neighborhood name Élysée to Elysée\n",
    "paris_data.loc[paris_data.Neighborhood == \"Élysée\", [\"Neighborhood\"]] = \"Elysée\"\n",
    "\n",
    "paris_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use GeoPy library to get the latitude and longitude values of Paris\n",
    "\n",
    "address = \"Paris, FR\"\n",
    "\n",
    "geolocator = Nominatim(user_agent=\"my-application\")\n",
    "location_par = geolocator.geocode(address)\n",
    "latitude_par = location_par.latitude\n",
    "longitude_par = location_par.longitude\n",
    "\n",
    "print(\"The geograpical coordinates of Paris are {}, {}.\".format(latitude_par, longitude_par))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a leaflet map of Paris with arrondissements superimposed on top\n",
    "\n",
    "# create a map of Paris using latitude and longitude values\n",
    "map_paris = folium.Map(location=[latitude_par, longitude_par], zoom_start=12)\n",
    "\n",
    "# add markers to the map\n",
    "for lat, lng, arrondissement, neighborhood in zip(paris_data[\"Latitude\"], paris_data[\"Longitude\"], paris_data[\"Arrondissement\"], paris_data[\"Neighborhood\"]):\n",
    "    label = \"{}, {}\".format(arrondissement,neighborhood)\n",
    "    label = folium.Popup(label, parse_html=True)\n",
    "    folium.CircleMarker(\n",
    "        [lat, lng],\n",
    "        radius = 5,\n",
    "        popup = label,\n",
    "        color = \"blue\",\n",
    "        fill = True,\n",
    "        fill_color = \"#3186cc\",\n",
    "        fill_opacity = 0.7,\n",
    "        parse_html = False).add_to(map_paris)  \n",
    "    \n",
    "map_paris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data about New York geography from https://ibm.box.com/shared/static/fbpwbovar7lf8p5sgddm06cgipa2rxpe.json\n",
    "\n",
    "with urllib.request.urlopen(\"https://ibm.box.com/shared/static/fbpwbovar7lf8p5sgddm06cgipa2rxpe.json\") as file:\n",
    "   newyork_data = json.loads(file.read())\n",
    "#newyork_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tranform the json data into a formatted Pandas dataframe\n",
    "\n",
    "# all the relevant data is in the features key, which is basically a list of the neighborhoods\n",
    "neighborhoods_data = newyork_data[\"features\"]\n",
    "\n",
    "# define the dataframe columns\n",
    "column_names = [\"Borough\", \"Neighborhood\", \"Latitude\", \"Longitude\"] \n",
    "\n",
    "# instantiate the dataframe\n",
    "neighborhoods = pd.DataFrame(columns=column_names)\n",
    "\n",
    "# loop through the data and fill the dataframe one row at a time\n",
    "for data in neighborhoods_data:\n",
    "    borough = neighborhood_name = data[\"properties\"][\"borough\"] \n",
    "    neighborhood_name = data[\"properties\"][\"name\"]\n",
    "        \n",
    "    neighborhood_latlon = data[\"geometry\"][\"coordinates\"]\n",
    "    neighborhood_lat = neighborhood_latlon[1]\n",
    "    neighborhood_lon = neighborhood_latlon[0]\n",
    "    \n",
    "    neighborhoods = neighborhoods.append({\"Borough\": borough,\n",
    "                                          \"Neighborhood\": neighborhood_name,\n",
    "                                          \"Latitude\": neighborhood_lat,\n",
    "                                          \"Longitude\": neighborhood_lon}, ignore_index=True)\n",
    "    \n",
    "neighborhoods.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manhattan_data = neighborhoods[neighborhoods[\"Borough\"] == \"Manhattan\"].reset_index(drop=True)\n",
    "manhattan_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "address = \"Manhattan, NY\"\n",
    "\n",
    "geolocator = Nominatim(user_agent=\"my-application\")\n",
    "location_man = geolocator.geocode(address)\n",
    "latitude_man = location_man.latitude\n",
    "longitude_man = location_man.longitude\n",
    "\n",
    "print(\"The geograpical coordinates of Manhattan are {}, {}.\".format(latitude_man, longitude_man))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a leaflet map of Manhattan with neighborhoods superimposed on top\n",
    "\n",
    "# create a map of Manhattan using latitude and longitude values\n",
    "map_manhattan = folium.Map(location=[latitude_man, longitude_man], zoom_start=11)\n",
    "\n",
    "# add markers to the map\n",
    "for lat, lng, neighborhood in zip(manhattan_data[\"Latitude\"], manhattan_data[\"Longitude\"], manhattan_data[\"Neighborhood\"]):\n",
    "    label = \"{}\".format(neighborhood)\n",
    "    label = folium.Popup(label, parse_html=True)\n",
    "    folium.CircleMarker(\n",
    "        [lat, lng],\n",
    "        radius = 5,\n",
    "        popup = label,\n",
    "        color = \"red\",\n",
    "        fill = True,\n",
    "        fill_color = \"#E03450\",\n",
    "        fill_opacity = 0.7,\n",
    "        parse_html = False).add_to(map_manhattan)  \n",
    "    \n",
    "map_manhattan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLIENT_ID = \"MF4VNN5LCLCUG1F01NH2JR2SJ0ILYOGLVCDHD2KGNXIBTT1U\" \n",
    "CLIENT_SECRET = \"GQ04P00OKBMLX0EUM0FGO5JOMA44JFSAEUOTFYGZY35XOSSC\" \n",
    "VERSION = \"20180605\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to gather recommended venues, with specifically the name and category, using the explore API\n",
    "\n",
    "def getNearbyVenues_paris(arrondissements, names, latitudes, longitudes, radius=750, limit=100):\n",
    "    \n",
    "    venues_list = []\n",
    "    for arrondissement, name, lat, lng in zip(arrondissements, names, latitudes, longitudes):\n",
    "        print(name)\n",
    "            \n",
    "        # create the API request URL\n",
    "        url = \"https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}\".format(\n",
    "            CLIENT_ID, \n",
    "            CLIENT_SECRET, \n",
    "            VERSION, \n",
    "            lat, \n",
    "            lng, \n",
    "            radius, \n",
    "            limit)\n",
    "            \n",
    "        # make the GET request\n",
    "        results = requests.get(url).json()[\"response\"][\"groups\"][0][\"items\"]\n",
    "        \n",
    "        # return only relevant information for each nearby venue\n",
    "        venues_list.append([(\n",
    "            arrondissement,\n",
    "            name, \n",
    "            lat, \n",
    "            lng, \n",
    "            v[\"venue\"][\"name\"], \n",
    "            v[\"venue\"][\"location\"][\"lat\"], \n",
    "            v[\"venue\"][\"location\"][\"lng\"],  \n",
    "               v[\"venue\"][\"categories\"][0][\"name\"]) for v in results])\n",
    "\n",
    "    nearby_venues = pd.DataFrame([item for venue_list in venues_list for item in venue_list])\n",
    "    nearby_venues.columns = [\"Arrondissement\",\n",
    "                  \"Neighborhood\", \n",
    "                  \"Neighborhood Latitude\", \n",
    "                  \"Neighborhood Longitude\", \n",
    "                  \"Venue\", \n",
    "                  \"Venue Latitude\", \n",
    "                  \"Venue Longitude\", \n",
    "                  \"Venue Category\"]\n",
    "    \n",
    "    return(nearby_venues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new dataframe called paris_venues that combines the recommended venues from Foursquare \n",
    "# and neigborhoods geo-information from the city dataset\n",
    "\n",
    "paris_venues = getNearbyVenues_paris(arrondissements=paris_data[\"Arrondissement\"],\n",
    "                               names = paris_data[\"Neighborhood\"],\n",
    "                               latitudes = paris_data[\"Latitude\"],\n",
    "                               longitudes = paris_data[\"Longitude\"]\n",
    "                               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(paris_venues.shape)\n",
    "print(\"There are {} unique categories.\".format(len(paris_venues[\"Venue Category\"].unique())))\n",
    "paris_venues.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to gather recommended venues, with specifically the name and category, using the explore API\n",
    "\n",
    "def getNearbyVenues_manhattan(boroughs, names, latitudes, longitudes, radius=750, limit=100):\n",
    "    \n",
    "    venues_list = []\n",
    "    for borough, name, lat, lng in zip(boroughs, names, latitudes, longitudes):\n",
    "        print(name)\n",
    "            \n",
    "        # create the API request URL\n",
    "        url = \"https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}\".format(\n",
    "            CLIENT_ID, \n",
    "            CLIENT_SECRET, \n",
    "            VERSION, \n",
    "            lat, \n",
    "            lng, \n",
    "            radius, \n",
    "            limit)\n",
    "            \n",
    "        # make the GET request\n",
    "        results = requests.get(url).json()[\"response\"][\"groups\"][0][\"items\"]\n",
    "        \n",
    "        # return only relevant information for each nearby venue\n",
    "        venues_list.append([(\n",
    "            borough,\n",
    "            name, \n",
    "            lat, \n",
    "            lng, \n",
    "            v[\"venue\"][\"name\"], \n",
    "            v[\"venue\"][\"location\"][\"lat\"], \n",
    "            v[\"venue\"][\"location\"][\"lng\"],  \n",
    "            v[\"venue\"][\"categories\"][0][\"name\"]) for v in results])\n",
    "\n",
    "    nearby_venues = pd.DataFrame([item for venue_list in venues_list for item in venue_list])\n",
    "    nearby_venues.columns = [\"Borough\",\n",
    "                  \"Neighborhood\", \n",
    "                  \"Neighborhood Latitude\", \n",
    "                  \"Neighborhood Longitude\", \n",
    "                  \"Venue\", \n",
    "                  \"Venue Latitude\", \n",
    "                  \"Venue Longitude\", \n",
    "                  \"Venue Category\"]\n",
    "    \n",
    "    return(nearby_venues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new dataframe called manhattan_venues that combines the recommended venues from Foursquare \n",
    "# and neigborhoods geo-information from the city dataset\n",
    "\n",
    "manhattan_venues = getNearbyVenues_manhattan(boroughs=manhattan_data[\"Borough\"],\n",
    "                               names = manhattan_data[\"Neighborhood\"],\n",
    "                               latitudes = manhattan_data[\"Latitude\"],\n",
    "                               longitudes = manhattan_data[\"Longitude\"]\n",
    "                               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(manhattan_venues.shape)\n",
    "print(\"There are {} unique categories.\".format(len(manhattan_venues[\"Venue Category\"].unique())))\n",
    "manhattan_venues.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paris_onehot = pd.get_dummies(paris_venues[[\"Venue Category\"]], prefix=\"\", prefix_sep=\"\")\n",
    "\n",
    "# add arrondissement and neighborhood column back to dataframe\n",
    "paris_onehot[\"Arrondissement\"] = paris_venues[\"Arrondissement\"] \n",
    "paris_onehot[\"Neighborhood\"] = paris_venues[\"Neighborhood\"] \n",
    "\n",
    "# move arrondissement and neighborhood columns to the first columns\n",
    "fixed_columns = [paris_onehot.columns[-2], paris_onehot.columns[-1]] + list(paris_onehot.columns[:-2])\n",
    "paris_onehot = paris_onehot[fixed_columns]\n",
    "\n",
    "paris_onehot.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by neighborhood and by the mean of frequency for each category\n",
    "\n",
    "paris_grouped = paris_onehot.groupby(\"Neighborhood\").mean().reset_index()\n",
    "paris_grouped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 5 most common categories of venues for each neighborhood \n",
    "\n",
    "num_top_venues = 5\n",
    "\n",
    "for hood in paris_grouped[\"Neighborhood\"]:\n",
    "    print(\"----\"+hood+\"----\")\n",
    "    temp = paris_grouped[paris_grouped[\"Neighborhood\"]==hood].T.reset_index()\n",
    "    temp.columns = [\"venue\",\"freq\"]\n",
    "    temp = temp.iloc[2:]\n",
    "    temp[\"freq\"] = temp[\"freq\"].astype(float)\n",
    "    temp = temp.round({\"freq\": 2})\n",
    "    print(temp.sort_values(\"freq\", ascending=False).reset_index(drop=True).head(num_top_venues))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_most_common_venues(row, num_top_venues):\n",
    "    row_categories = row.iloc[1:]\n",
    "    row_categories_sorted = row_categories.sort_values(ascending=False)\n",
    "    \n",
    "    return row_categories_sorted.index.values[0:num_top_venues]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new dataframe and display the top 5 venues for each neighborhood\n",
    "\n",
    "num_top_venues = 10\n",
    "\n",
    "indicators = [\"st\", \"nd\", \"rd\"]\n",
    "\n",
    "# create columns according to number of top venues\n",
    "columns = [\"Arrondissement\", \"Neighborhood\"]\n",
    "for ind in np.arange(num_top_venues):\n",
    "    try:\n",
    "        columns.append(\"{}{} Most Common Venue\".format(ind+1, indicators[ind]))\n",
    "    except:\n",
    "        columns.append(\"{}th Most Common Venue\".format(ind+1))\n",
    "\n",
    "# create a new dataframe\n",
    "paris_neighborhoods_venues_sorted = pd.DataFrame(columns=columns)\n",
    "paris_neighborhoods_venues_sorted[[ \"Arrondissement\",\"Neighborhood\"]] = paris_grouped[[\"Arrondissement\",\"Neighborhood\"]]\n",
    "\n",
    "for ind in np.arange(paris_grouped.shape[0]):\n",
    "    paris_neighborhoods_venues_sorted.iloc[ind, 2:] = return_most_common_venues(paris_grouped.iloc[ind, 1:], num_top_venues)\n",
    "\n",
    "paris_neighborhoods_venues_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encoding\n",
    "\n",
    "manhattan_onehot = pd.get_dummies(manhattan_venues[[\"Venue Category\"]], prefix=\"\", prefix_sep=\"\")\n",
    "\n",
    "# add arrondissement and neighborhood column back to dataframe\n",
    "manhattan_onehot[\"Neighborhood\"] = manhattan_venues[\"Neighborhood\"] \n",
    "\n",
    "# move arrondissement and neighborhood columns to the first columns\n",
    "fixed_columns = [manhattan_onehot.columns[-1]] + list(manhattan_onehot.columns[:-1])\n",
    "manhattan_onehot = manhattan_onehot[fixed_columns]\n",
    "\n",
    "manhattan_onehot.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by neighborhood and by the mean of frequency for each category\n",
    "\n",
    "manhattan_grouped = manhattan_onehot.groupby(\"Neighborhood\").mean().reset_index()\n",
    "manhattan_grouped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_top_venues = 5\n",
    "\n",
    "for hood in manhattan_grouped[\"Neighborhood\"]:\n",
    "    print(\"----\"+hood+\"----\")\n",
    "    temp = manhattan_grouped[manhattan_grouped[\"Neighborhood\"]==hood].T.reset_index()\n",
    "    temp.columns = [\"venue\",\"freq\"]\n",
    "    temp = temp.iloc[1:]\n",
    "    temp[\"freq\"] = temp[\"freq\"].astype(float)\n",
    "    temp = temp.round({\"freq\": 2})\n",
    "    print(temp.sort_values('freq', ascending=False).reset_index(drop=True).head(num_top_venues))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_top_venues = 10\n",
    "\n",
    "indicators = [\"st\", \"nd\", \"rd\"]\n",
    "\n",
    "# create columns according to number of top venues\n",
    "columns = [\"Neighborhood\"]\n",
    "for ind in np.arange(num_top_venues):\n",
    "    try:\n",
    "        columns.append(\"{}{} Most Common Venue\".format(ind+1, indicators[ind]))\n",
    "    except:\n",
    "        columns.append(\"{}th Most Common Venue\".format(ind+1))\n",
    "\n",
    "# create a new dataframe\n",
    "manhattan_neighborhoods_venues_sorted = pd.DataFrame(columns=columns)\n",
    "manhattan_neighborhoods_venues_sorted[[\"Neighborhood\"]] = manhattan_grouped[[\"Neighborhood\"]]\n",
    "\n",
    "for ind in np.arange(manhattan_grouped.shape[0]):\n",
    "    manhattan_neighborhoods_venues_sorted.iloc[ind, 1:] = return_most_common_venues(manhattan_grouped.iloc[ind, :], num_top_venues)\n",
    "\n",
    "manhattan_neighborhoods_venues_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the metric (same squared distances) to identify the best k\n",
    "\n",
    "columns = [\"Neighborhood\", \"Arrondissement\"]\n",
    "paris_grouped_clustering = paris_grouped.drop(columns, axis = 1)\n",
    "\n",
    "Sum_of_squared_distances = []\n",
    "\n",
    "ks = range(1,11)\n",
    "for k in ks:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=0).fit(paris_grouped_clustering)\n",
    "    Sum_of_squared_distances.append(kmeans.inertia_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot of sum of squared distances\n",
    "\n",
    "plt.plot(ks, Sum_of_squared_distances, \"bx-\")\n",
    "plt.xlabel(\"k number of clusters\")\n",
    "plt.ylabel(\"Sum_of_squared_distances\")\n",
    "plt.title(\"Elbow method for optimal k\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster the venues of Paris into 5 clusters\n",
    "\n",
    "# set number of clusters\n",
    "kclusters = 5\n",
    "\n",
    "# run k-means clustering\n",
    "kmeans = KMeans(n_clusters=kclusters, random_state=0).fit(paris_grouped_clustering)\n",
    "\n",
    "# check cluster labels generated for each row in the dataframe\n",
    "kmeans.labels_[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paris_merged = paris_data\n",
    " \n",
    "# add clustering labels\n",
    "paris_merged[\"Cluster Labels\"] = kmeans.labels_\n",
    "paris_merged = pd.merge(paris_merged, paris_neighborhoods_venues_sorted,  how = \"left\", left_on = [\"Arrondissement\",\"Neighborhood\"], right_on = [\"Arrondissement\",\"Neighborhood\"])\n",
    "\n",
    "paris_merged\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create map\n",
    "map_clusters = folium.Map(location=[latitude_par, longitude_par], zoom_start=12)\n",
    "\n",
    "# set color scheme for the clusters\n",
    "x = np.arange(kclusters)\n",
    "ys = [i+x+(i*x)**2 for i in range(kclusters)]\n",
    "colors_array = cm.rainbow(np.linspace(0, 1, len(ys)))\n",
    "rainbow = [colors.rgb2hex(i) for i in colors_array]\n",
    "\n",
    "# add markers to the map\n",
    "markers_colors = []\n",
    "for lat, lon, poi, cluster in zip(paris_merged[\"Latitude\"], paris_merged[\"Longitude\"], paris_merged[\"Neighborhood\"], paris_merged[\"Cluster Labels\"]):\n",
    "    label = folium.Popup(str(poi) + \"Cluster\" + str(cluster), parse_html=True)\n",
    "    folium.CircleMarker(\n",
    "        [lat, lon],\n",
    "        radius = 5,\n",
    "        popup = label,\n",
    "        color = rainbow[cluster-1],\n",
    "        fill = True,\n",
    "        fill_color = rainbow[cluster-1],\n",
    "        fill_opacity = 0.7).add_to(map_clusters)\n",
    "       \n",
    "map_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the metric (same squared distances) to identify the best k\n",
    "\n",
    "columns = [\"Neighborhood\"]\n",
    "manhattan_grouped_clustering = manhattan_grouped.drop(columns, axis = 1)\n",
    "\n",
    "Sum_of_squared_distances = []\n",
    "\n",
    "ks = range(1,11)\n",
    "for k in ks:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=0).fit(manhattan_grouped_clustering)\n",
    "    Sum_of_squared_distances.append(kmeans.inertia_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot of sum of squared distances\n",
    "\n",
    "plt.plot(ks, Sum_of_squared_distances, \"bx-\")\n",
    "plt.xlabel(\"k number of clusters\")\n",
    "plt.ylabel(\"Sum_of_squared_distances\")\n",
    "plt.title(\"Elbow method for optimal k\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set number of clusters\n",
    "kclusters = 8\n",
    "\n",
    "# run k-means clustering\n",
    "kmeans = KMeans(n_clusters=kclusters, random_state=0).fit(manhattan_grouped_clustering)\n",
    "\n",
    "# check cluster labels generated for each row in the dataframe\n",
    "kmeans.labels_[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manhattan_merged = manhattan_data\n",
    "\n",
    "# add clustering labels\n",
    "manhattan_merged[\"Cluster Labels\"] = kmeans.labels_\n",
    "\n",
    "# merge manhattan_grouped with manhattann_data to add latitude/longitude for each neighborhood\n",
    "manhattan_merged = manhattan_merged.join(manhattan_neighborhoods_venues_sorted.set_index(\"Neighborhood\"), on=\"Neighborhood\")\n",
    "\n",
    "manhattan_merged # check the last columns!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_clusters = folium.Map(location=[latitude_man, longitude_man], zoom_start=11)\n",
    "\n",
    "# set color scheme for the clusters\n",
    "x = np.arange(kclusters)\n",
    "ys = [i+x+(i*x)**2 for i in range(kclusters)]\n",
    "colors_array = cm.rainbow(np.linspace(0, 1, len(ys)))\n",
    "rainbow = [colors.rgb2hex(i) for i in colors_array]\n",
    "\n",
    "# add markers to the map\n",
    "markers_colors = []\n",
    "for lat, lon, poi, cluster in zip(manhattan_merged[\"Latitude\"], manhattan_merged[\"Longitude\"], manhattan_merged[\"Neighborhood\"], manhattan_merged[\"Cluster Labels\"]):\n",
    "    label = folium.Popup(str(poi) + \"Cluster\" + str(cluster), parse_html=True)\n",
    "    folium.CircleMarker(\n",
    "        [lat, lon],\n",
    "        radius = 5,\n",
    "        popup = label,\n",
    "        color = rainbow[cluster-1],\n",
    "        fill = True,\n",
    "        fill_color = rainbow[cluster-1],\n",
    "        fill_opacity = 0.7).add_to(map_clusters)\n",
    "       \n",
    "map_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster 1\n",
    "\n",
    "paris_merged.loc[paris_merged[\"Cluster Labels\"] == 0, paris_merged.columns[[1] + list(range(5, paris_merged.shape[1]))]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster 2\n",
    "\n",
    "paris_merged.loc[paris_merged[\"Cluster Labels\"] == 1, paris_merged.columns[[1] + list(range(5, paris_merged.shape[1]))]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster 3\n",
    "\n",
    "paris_merged.loc[paris_merged[\"Cluster Labels\"] == 2, paris_merged.columns[[1] + list(range(5, paris_merged.shape[1]))]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster 4\n",
    "\n",
    "paris_merged.loc[paris_merged[\"Cluster Labels\"] == 3, paris_merged.columns[[1] + list(range(5, paris_merged.shape[1]))]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster 5\n",
    "\n",
    "paris_merged.loc[paris_merged[\"Cluster Labels\"] == 4, paris_merged.columns[[1] + list(range(5, paris_merged.shape[1]))]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster 1\n",
    "\n",
    "manhattan_merged.loc[manhattan_merged[\"Cluster Labels\"] == 0, manhattan_merged.columns[[1] + list(range(5, manhattan_merged.shape[1]))]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster 2\n",
    "\n",
    "manhattan_merged.loc[manhattan_merged[\"Cluster Labels\"] == 1, manhattan_merged.columns[[1] + list(range(5, manhattan_merged.shape[1]))]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster 3\n",
    "\n",
    "manhattan_merged.loc[manhattan_merged[\"Cluster Labels\"] == 2, manhattan_merged.columns[[1] + list(range(5, manhattan_merged.shape[1]))]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster 4\n",
    "\n",
    "manhattan_merged.loc[manhattan_merged[\"Cluster Labels\"] == 3, manhattan_merged.columns[[1] + list(range(5, manhattan_merged.shape[1]))]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manhattan_merged.loc[manhattan_merged[\"Cluster Labels\"] == 4, manhattan_merged.columns[[1] + list(range(5, manhattan_merged.shape[1]))]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster 6\n",
    "\n",
    "manhattan_merged.loc[manhattan_merged[\"Cluster Labels\"] == 5, manhattan_merged.columns[[1] + list(range(5, manhattan_merged.shape[1]))]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manhattan_merged.loc[manhattan_merged[\"Cluster Labels\"] == 6, manhattan_merged.columns[[1] + list(range(5, manhattan_merged.shape[1]))]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster 8\n",
    "\n",
    "manhattan_merged.loc[manhattan_merged[\"Cluster Labels\"] == 7, manhattan_merged.columns[[1] + list(range(5, manhattan_merged.shape[1]))]]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "conda-env-python-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
